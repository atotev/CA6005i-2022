{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcbd558a",
   "metadata": {},
   "source": [
    "## TODO\n",
    "#### 0. Create the actual search engine\n",
    "#### 1. Process document titles and query descriptions?\n",
    "#### 2. Make solution case sensitive?\n",
    "#### 3. Handle named entities?\n",
    "#### 4. Computational improvements? (streaming parser, opt. cosine similarity, sparse matrices)\n",
    "#### 5. Other text pre-processing\n",
    "#### 6. Use OOP (particularly for inverted index, but other entities, too)\n",
    "#### 7. Different b, k1 for BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f9cc375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/atotev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/atotev/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/atotev/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
     ]
    }
   ],
   "source": [
    "from os import walk, path\n",
    "from lxml import etree\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff93f2",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a35ac365",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_dir = '../COLLECTION'\n",
    "topics_dir = '../topics'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767fbc6a",
   "metadata": {},
   "source": [
    "### Create the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "769cfc3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12208/12208 [06:31<00:00, 31.16it/s] \n"
     ]
    }
   ],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words(InvertedIndex.LANGUAGE))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def count_terms(self, text):\n",
    "        if not text:\n",
    "            return Counter()\n",
    "        f = lambda word: InvertedIndex.TERM_FILTER_REGEX.match(word) and word not in self.stop_words\n",
    "        lemmatized = map(self.lemmatizer.lemmatize, text.lower().split())\n",
    "        return Counter(filter(f, lemmatized))\n",
    "\n",
    "class FileLoader:\n",
    "    ELMNT_DOCID = 'DOCID'\n",
    "    ELMNT_TEXT = 'TEXT'\n",
    "    ELMNT_QUERYID = 'QUERYID'\n",
    "    ELMNT_TITLE = 'TITLE'\n",
    "    \n",
    "    def load_document(self, filepath):\n",
    "        tree = etree.parse(filepath)\n",
    "        return (tree.find(FileLoader.ELMNT_DOCID).text, tree.find(FileLoader.ELMNT_TEXT).text)\n",
    "\n",
    "    def load_query_file(self, filepath):\n",
    "        tree = etree.parse(filepath)\n",
    "        return (tree.find(FileLoader.ELMNT_QUERYID).text, tree.find(FileLoader.ELMNT_TITLE).text)\n",
    "                           \n",
    "class Posting:\n",
    "    def __init__(self, docid, count):\n",
    "        self.docid = docid\n",
    "        self.count = count\n",
    "\n",
    "class InvertedIndex:\n",
    "    LANGUAGE = 'english'\n",
    "    TERM_FILTER_REGEX = re.compile('^[a-z][a-z_\\\\-]*[a-z]$') # length>1, no punctuation-only\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._document_count = 0\n",
    "        self._documents_total_length = 0\n",
    "        self._index = {}\n",
    "        self._document_files = {}\n",
    "        self._file_loader = FileLoader()\n",
    "        self._test_proc = TextProcessor()\n",
    "        \n",
    "    def _add_postings(self, docid, text):\n",
    "        tcounts = self._test_proc.count_terms(text)\n",
    "        for t in tcounts:\n",
    "            if t not in self._index:\n",
    "                self._index[t] = []\n",
    "            self._index[t].append(Posting(docid, tcounts[t]))\n",
    "        return tcounts\n",
    "\n",
    "    def add_files(self, document_dir):\n",
    "        all_documents_length = 0\n",
    "        basepath, _, files = next(walk(document_dir))\n",
    "        for each in tqdm(files):\n",
    "            filepath = path.join(document_dir, each)\n",
    "            docid, text = self._file_loader.load_document(filepath)\n",
    "            self._document_files[docid] = filepath\n",
    "            term_counts = self._add_postings(docid, text)\n",
    "            self._documents_total_length += sum(term_counts.values())\n",
    "            self._document_count += 1\n",
    "    \n",
    "    def get_avg_document_length(self):\n",
    "        return self._documents_total_length / self._document_count\n",
    "    \n",
    "    def get_documents_total_length(self):\n",
    "        return self._documents_total_length\n",
    "            \n",
    "    def keys(self):\n",
    "        return self._index.keys()\n",
    "    \n",
    "    def get_posting_list(self, term):\n",
    "        return self._index[term]\n",
    "    \n",
    "    def get_document_file(self, docid):\n",
    "        return self._document_files[docid]\n",
    "    \n",
    "    def get_document_count(self):\n",
    "        return self._document_count\n",
    "                \n",
    "index = InvertedIndex()\n",
    "index.add_files(collection_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79a3f8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 10.2452/141-AH:  Letter Bomb for Kiesbauer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [00:13<00:55, 13.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 10.2452/142-AH:  Christo wraps German Reichstag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:21<00:30, 10.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 10.2452/143-AH:  Women ' s Conference Beijing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [00:50<00:37, 18.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 10.2452/144-AH:  Sierra_Leone Rebellion and Diamonds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [00:53<00:12, 12.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 10.2452/145-AH:  Japanese Rice Imports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:06<00:00, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map                   \tall\t0.4982\r\n",
      "P_5                   \tall\t0.3200\r\n",
      "P_10                  \tall\t0.2200\r\n",
      "P_15                  \tall\t0.2000\r\n",
      "P_20                  \tall\t0.1700\r\n",
      "P_30                  \tall\t0.1133\r\n",
      "P_100                 \tall\t0.0480\r\n",
      "P_200                 \tall\t0.0290\r\n",
      "P_500                 \tall\t0.0120\r\n",
      "P_1000                \tall\t0.0062\r\n",
      "ndcg                  \tall\t0.5653\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class SearchResult:\n",
    "    def __init__(self, docid):\n",
    "        self.docid = docid\n",
    "        self.terms = Counter()\n",
    "        self.relevance = 0.\n",
    "        self.custom_data = {}\n",
    "\n",
    "class RankingStrategy(ABC):\n",
    "    @abstractmethod\n",
    "    def set_param(self, name, value):\n",
    "        raise Exception('Abstract method call attempted')\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_rank(self, search_result, query_terms, qt):\n",
    "        raise Exception('Abstract method call attempted')\n",
    "\n",
    "class Bm25Ranking(RankingStrategy):\n",
    "    PARAM_B = 'bm25.b'\n",
    "    PARAM_K1 = 'bm25.k1'\n",
    "    \n",
    "    def __init__(self, index):\n",
    "        self._index = index\n",
    "        self._config = { Bm25Ranking.PARAM_B: 0.75, Bm25Ranking.PARAM_K1: 1.25 }\n",
    "    \n",
    "    def set_param(self, name, value):\n",
    "        self._config[name] = value\n",
    "        \n",
    "    def update_rank(self, search_result, query_terms, qt):\n",
    "        if qt not in search_result.terms:\n",
    "            return\n",
    "        \n",
    "        tf = search_result.terms[qt]\n",
    "        N = self._index.get_document_count()\n",
    "        n = len(self._index.get_posting_list(qt))\n",
    "        dl = sum(search_result.terms.values())\n",
    "        avdl = self._index.get_avg_document_length()\n",
    "        k1 = self._config[Bm25Ranking.PARAM_K1]\n",
    "        b = self._config[Bm25Ranking.PARAM_B]\n",
    "        search_result.relevance += tf / ((k1 * (1 - b + (b * dl / avdl))) + tf) * np.log((N - n + 0.5) / (n + 0.5))\n",
    "        \n",
    "class LmJmsRanking(RankingStrategy):\n",
    "    PARAM_LAMBDA = 'lmjms.lambda'\n",
    "    \n",
    "    def __init__(self, index):\n",
    "        self._index = index\n",
    "        self._config = { LmJmsRanking.PARAM_LAMBDA: 0.25 }\n",
    "    \n",
    "    def set_param(self, name, value):\n",
    "        self._config[name] = value\n",
    "        \n",
    "    def update_rank(self, search_result, query_terms, qt):\n",
    "        Cd = search_result.terms[qt]\n",
    "        sumCd = sum(search_result.terms.values())\n",
    "        CD = sum(p.count for p in self._index.get_posting_list(qt))\n",
    "        sumCD = self._index.get_documents_total_length()\n",
    "        lmbda = self._config[LmJmsRanking.PARAM_LAMBDA]\n",
    "        search_result.relevance += np.log(((1 - lmbda) * Cd / sumCd) + (lmbda * CD / sumCD))\n",
    "\n",
    "class Search:\n",
    "    PARAM_ACTIVE = 'active'\n",
    "\n",
    "    IR_VSM = 'vsm'\n",
    "    IR_BM25 = 'bm25'\n",
    "    IR_LM = 'lm'\n",
    "    \n",
    "    RESULT_LIST_SIZE = 1000\n",
    "    \n",
    "    def __init__(self, index):\n",
    "        self._index = index\n",
    "        self._irModels = {\n",
    "            Search.IR_VSM: VsmRanking(self._index),\n",
    "            Search.IR_BM25: Bm25Ranking(self._index),\n",
    "            Search.IR_LM: LmJmsRanking(self._index)\n",
    "        }\n",
    "        self._ACTIVE_RANKING = self._irModels[Search.IR_VSM]\n",
    "        self._file_loader = FileLoader()\n",
    "        self._text_proc = TextProcessor()\n",
    "        \n",
    "    def _count_terms(self, docid):\n",
    "        filepath = self._index.get_document_file(docid)\n",
    "        _, text = self._file_loader.load_document(filepath)\n",
    "        return self._text_proc.count_terms(text)\n",
    "    \n",
    "    def _remove_unkown(self, term_counts):\n",
    "        return Counter({x: count for x, count in term_counts.items() if x in self._index.keys()})\n",
    "    \n",
    "    def execute(self, query_text):\n",
    "        search_results = {}\n",
    "        query_terms = self._text_proc.count_terms(query_text)\n",
    "        query_terms = self._remove_unkown(query_terms)\n",
    "        for qt in query_terms:\n",
    "            posting_list = self._index.get_posting_list(qt)\n",
    "            for p in posting_list:\n",
    "                if p.docid not in search_results:\n",
    "                    sr = SearchResult(p.docid)\n",
    "                    sr.terms = self._count_terms(p.docid)\n",
    "                    search_results[p.docid] = sr\n",
    "        for qt in query_terms:\n",
    "            for docid in search_results:\n",
    "                self._ACTIVE_RANKING.update_rank(search_results[docid], query_terms, qt)\n",
    "        result = list(search_results.values())\n",
    "        result.sort(reverse=True, key=lambda sr: sr.relevance)\n",
    "        return result[:Search.RESULT_LIST_SIZE]\n",
    "\n",
    "    def configure(self, param_name, param_value):\n",
    "        if Search.PARAM_ACTIVE!=param_name:\n",
    "            self._ACTIVE_RANKING.set_param(param_name, param_value)\n",
    "        elif param_value in self._irModels:\n",
    "            self._ACTIVE_RANKING = self._irModels[param_value]\n",
    "        else:\n",
    "            raise Exception('Unrecognized ranking: %s' % (param_value))\n",
    "        return self\n",
    "\n",
    "search = Search(index).configure(Search.PARAM_ACTIVE, Search.IR_LM)\n",
    "\n",
    "\n",
    "class DocRank:\n",
    "    def __init__(self, qieryid, search_result):\n",
    "        self.queryid = queryid\n",
    "        self.sr = search_result\n",
    "        \n",
    "    def to_qrel(self):\n",
    "        return '%s Q0 %s rank %.6f STANDARD\\n' % (self.queryid, self.sr.docid, self.sr.relevance)\n",
    "\n",
    "queryids = set()\n",
    "ranks = []\n",
    "_, _, topic_files = next(walk(topics_dir))\n",
    "test_topic_files = np.array(topic_files)[:5]\n",
    "file_loader = FileLoader()\n",
    "for each in tqdm(test_topic_files):\n",
    "    queryid, query_text = file_loader.load_query_file(path.join(topics_dir, each))\n",
    "    queryids.add(queryid)\n",
    "    print('Processing topic %s: %s' % (queryid, query_text))\n",
    "    ranks.extend(map(lambda sr: DocRank(queryid, sr), search.execute(query_text)))\n",
    "        \n",
    "    \n",
    "def results_filepath():\n",
    "    return './results_%s.txt' % (datetime.now().strftime('%d-%m-%Y_%H-%M-%S'))\n",
    "results_file = results_filepath()\n",
    "with open(results_file, 'w') as fp:\n",
    "    fp.writelines(r.to_qrel() for r in ranks)\n",
    "\n",
    "qrels_file = '%s.qrels' % (results_file)\n",
    "with open(qrels_file, 'w') as fp:\n",
    "    with open('../test_qrels.txt') as qrelf:\n",
    "        fp.writelines(line for line in qrelf if any(qid in line for qid in queryids))\n",
    "\n",
    "!{'../trec_eval-9.0.7/trec_eval -m ndcg -m map -m P %s %s' % (qrels_file, results_file)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
